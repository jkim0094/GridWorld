{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5226 Project Stage 1\n",
    "\n",
    "## Group 8\n",
    "- **Members:**\n",
    "  - Jeeeun Kim\n",
    "  - Jinxu Tao\n",
    "  - Xiaolong Shen\n",
    "  - Zhihan Ye\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Introduction\n",
    "This project aims to develop a table-based Q-learning agent to solve a simple transport task in a grid world environment. The agent's goal is to pick up an item located at a random position (A) and deliver it to a fixed goal location (B) with the minimum number of steps possible.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Environment Setup\n",
    "The environment is implemented as an `n x n` grid world. The agent and the item are placed at random locations within this grid, while the goal location is fixed at the bottom-right corner of the grid. The main components of the environment are as follows:\n",
    "\n",
    "- **Grid Size (n):** Initially set to 5x5 but can be configured to other sizes.\n",
    "- **Agent's Position:** Randomly initialized starting position within the grid.\n",
    "- **Item Location (A):** Randomly placed at a different location from both the agent's starting position and the goal location.\n",
    "- **Goal Location (B):** Fixed at the bottom-right corner of the grid (coordinate (n, n)).\n",
    "- **State Space:** Defined by the agent's position (`n x n`), the item's position (`n x n`), and whether the agent is carrying the item (2).\n",
    "- **Action Space:** 'n', 's', 'e', 'w' (north, south, east, west).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Reward Structure\n",
    "The agent receives rewards and penalties based on its proximity to the item and the goal. The rewards are structured as follows:\n",
    "\n",
    "- **Basic Penalty:** A small negative reward (-1) for each move to minimize the number of steps.\n",
    "- **Item Pickup Reward:** +5 for getting closer to the item when not carrying it, -5 for moving away from the item.\n",
    "- **Goal Proximity Reward:** +10 for getting closer to the goal while carrying the item, -10 for moving away from the goal.\n",
    "- **Item Delivery Reward:** +50 for successfully delivering the item to the goal, -50 for reaching the goal without the item.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Q-Learning Algorithm\n",
    "\n",
    "- **Bellman Equation for Q-Learning:** \n",
    "$$\n",
    "Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma \\max_{a'} Q(S_{t+1}, a') - Q(S_t, A_t)]\n",
    "$$\n",
    "\n",
    "- **Initialization:** Initialize the Q-table to 0 for all state-action pairs.\n",
    "- **Policy Selection:** Use an epsilon-greedy policy to balance exploration and exploitation.\n",
    "- **Q-Value Update:** Update Q-values using the Bellman equation based on rewards and estimates of future rewards.\n",
    "- **Alpha Decay (Learning Rate Decay):** Gradually decrease the learning rate (α) to enable faster learning initially and stabilize later. The learning rate decreases over episodes.\n",
    "- **Epsilon Decay (Exploration Rate Decay):** Gradually decrease the exploration rate (ϵ) to explore various actions initially and exploit the learned optimal policy later. The exploration rate decreases over episodes.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Training Procedure\n",
    "The agent is trained over a specified number of episodes. In each episode, the agent starts from a random position and attempts to complete the task. Q-values are updated after each step, and the agent's policy is refined to minimize the number of steps needed to complete the task.\n",
    "\n",
    "Metrics such as total reward, steps per episode, Q-value convergence, and policy stability are tracked to evaluate the agent's learning progress.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Evaluation\n",
    "The trained agent is evaluated over multiple episodes using the following metrics:\n",
    "\n",
    "1. **Total reward**\n",
    "2. **Steps per episode**\n",
    "3. **Exploration vs. Exploitation Ratio:** Frequency of exploring new actions versus utilizing known strategies.\n",
    "4. **Learning Curve Stability:** Variance in rewards over recent episodes to assess learning consistency.\n",
    "5. **Q-Value Convergence:** Tracking average Q-values to determine if the agent converges to an optimal policy.\n",
    "6. **Policy Stability:** Frequency of policy changes as learning progresses.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Visualization\n",
    "Learning and evaluation results are visualized using Matplotlib. Key visualizations include:\n",
    "\n",
    "1. **Dynamic Animations**\n",
    "2. **Graphs of Steps and Total Rewards:** Displaying steps taken and total rewards obtained per episode.\n",
    "3. **Exploration vs. Exploitation:** Showing the balance of exploration and exploitation over time.\n",
    "4. **Learning Curve Stability:** Visualizing reward variance over episodes.\n",
    "5. **Q-Value Convergence vs. Policy Stability:** A combined graph showing the relationship between Q-value convergence and policy stability.\n",
    "6. **Final Q-Table**\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Conclusion\n",
    "Learning and evaluation results are visualized using Matplotlib. Key visualizations include:\n",
    "\n",
    "1. **Steps per Episode and Total Rewards:** As episodes progress, the number of steps required to complete the task noticeably decreases, indicating that the agent is learning to navigate the environment more efficiently. The total rewards fluctuate initially but stabilize over time, suggesting that the agent is improving its policy to maximize rewards and minimize penalties.\n",
    "\n",
    "2. **Q-Value Convergence and Policy Stability:** The agent's learning rate (alpha) is initially set high (alpha=0.9) to facilitate rapid learning. As training progresses, the learning rate is gradually reduced using alpha_decay=0.995, allowing the Q-values to converge more smoothly without drastic changes. A minimum learning rate (alpha_min=0.01) ensures that even in later stages of training, the agent can still make small adjustments, helping it trust its existing knowledge and react less sensitively to new experiences. This strategy helps the Q-value convergence curve to flatten gradually as learning progresses, showing that the agent is effectively learning the optimal value function and developing a stable policy. Policy stability is initially variable, but it becomes more stable as training continues, indicating that the agent is refining its policy and making fewer changes as it approaches an optimal strategy.\n",
    "\n",
    "3. **Learning Curve Stability:** The variance in rewards per episode is an indicator of learning curve stability. High variance in the early stages indicates inconsistent performance as the agent explores different actions. Over time, the variance decreases significantly, demonstrating that the agent is learning consistently and stabilizing its performance.\n",
    "\n",
    "4. **Exploration vs. Exploitation Ratio:** The agent's initial exploration rate (epsilon) is set high at 0.99, encouraging it to actively explore various state-action pairs during the early stages of learning. This high initial value allows the agent to gather sufficient information about the environment. As episodes progress, epsilon gradually decreases according to the decay rate (epsilon_decay=0.995), until it reaches a minimum value of epsilon_min=0.01. This ensures that a certain level of exploration is maintained while allowing the agent to leverage its learned knowledge most of the time. This reduction helps the agent focus on understanding the environment and selecting optimal actions, moving towards optimizing performance by repeating effective actions rather than exploring new ones as training progresses.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import Slider, Button\n",
    "#from matphttp://localhost:8888/notebooks/Desktop/%5BFIT5226%5DMulti%20Agent/FIT5226_2024_S2/Stage1/group8.ipynb#Environmentlotlib.widgets import Slider, Button\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Grid class:\n",
    "    1. define the grid, set the gird size as 5 for demostration\n",
    "    2. set the position of agent and item; the flag of carrying\n",
    "'''\n",
    "\n",
    "class Grid:\n",
    "\n",
    "    '''\n",
    "    the initial attributes for Grid obj\n",
    "    n: grid size\n",
    "    target position: it's fixed and set at (n,n)\n",
    "    reset(): set the position of agent and item, and the state of carrying\n",
    "\n",
    "    ''' \n",
    "    def __init__(self, n=5):\n",
    "        self.n = n\n",
    "        self.target_position = (self.n-1, self.n-1)\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    '''\n",
    "    generate the random positon in the gird\n",
    "    '''\n",
    "    def reset_position(self):\n",
    "        y, x = random.randint(0, self.n-1), random.randint(0, self.n-1)\n",
    "        return y, x\n",
    "        \n",
    "\n",
    "    '''\n",
    "    set the position of agent and item; \n",
    "    set the flag of has_item and at_target\n",
    "\n",
    "    agent's position can't be same as item's position & \n",
    "    item's position can't be same as target's position\n",
    "\n",
    "    '''\n",
    "    def reset(self):\n",
    "        self.agent_position = self.reset_position()\n",
    "        self.item_position = self.reset_position()\n",
    "        self.has_item = False\n",
    "        self.at_target = False\n",
    "\n",
    "        # Ensure the item is not placed at the agent's initial position or the target position\n",
    "        while self.agent_position == self.item_position or self.item_position == self.target_position:\n",
    "            self.item_position = self.reset_position()\n",
    "\n",
    "\n",
    "        # previous distance between agent and item\n",
    "        self.perv_dist_item = self._dist(self.agent_position, self.item_position)\n",
    "        \n",
    "        # previous distance between agent and target\n",
    "        self.perv_dist_target = self._dist(self.agent_position, self.target_position)\n",
    "\n",
    "\n",
    "        return self.get_state()\n",
    "        \n",
    "\n",
    "    '''\n",
    "    the document says that \n",
    "        the agent konws its position, \n",
    "        the item's position and \n",
    "        item's state\n",
    "\n",
    "    get the current state of agent:\n",
    "        agent position\n",
    "        item position\n",
    "        has_item\n",
    "    '''\n",
    "    def get_state(self):\n",
    "        return (self.agent_position, self.item_position, self.has_item)\n",
    "        \n",
    "\n",
    "\n",
    "    '''\n",
    "    calculate the distance between position_1 and position_2 (agent and target/item)\n",
    "    '''\n",
    "    def _dist(self, loc1, loc2):\n",
    "        return abs(loc1[0] - loc2[0]) + abs(loc1[1] - loc2[1])\n",
    "\n",
    "\n",
    "    '''\n",
    "    check whether the agent has arrieved the target with the item\n",
    "    '''\n",
    "    def is_at_target(self):\n",
    "        return self.agent_position == self.target_position and self.has_item\n",
    "\n",
    "\n",
    "    '''\n",
    "    self.has_item = False (initial)\n",
    "\n",
    "    when the agent has no item, calculate the distance between the agent and the item\n",
    "    when the agent has the item, calculate teh distance between the agent and the target\n",
    "    '''\n",
    "    def update_distances(self):\n",
    "        if not self.has_item:\n",
    "            self.perv_dist_item = self._dist(self.agent_position, self.item_position)\n",
    "        else:\n",
    "            self.perv_dist_target = self._dist(self.agent_position, self.target_position)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Structure\n",
    "\n",
    "T-D method \n",
    "1 step ahead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "agent class:\n",
    "    agent's movement\n",
    "    reward principle \n",
    "    Q-learning process\n",
    "    \n",
    "'''\n",
    "class Agent:\n",
    "    '''\n",
    "    parameters of q-learning\n",
    "    gird\n",
    "    agent action\n",
    "    q-table\n",
    "    '''\n",
    "    def __init__(self, grid, actions, alpha=0.9, gamma=0.95, epsilon=0.99,\n",
    "                 epsilon_decay=0.995, epsilon_min=0.01,\n",
    "                 alpha_min=0.01, alpha_decay=0.995):\n",
    "        self.grid = grid\n",
    "        self.gamma = gamma\n",
    "\n",
    "\n",
    "        # update_parameters()\n",
    "        # epsilon - large number means large step for exploring \n",
    "        self.epsilon = epsilon\n",
    "        # epsilon decay - the decay rate of epsilon \n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        # epsilon min - the minimum number of epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "\n",
    "        # alpha - is used when updating the q value - learning rate\n",
    "        self.alpha = alpha\n",
    "        self.alpha_decay = alpha_decay\n",
    "        self.alpha_min = alpha_min\n",
    "\n",
    "\n",
    "        \n",
    "        self.actions = actions  # ['n', 's', 'e', 'w']\n",
    "        self.action_dir = {'n': (-1, 0), 's': (1, 0), 'e': (0, 1), 'w': (0, -1)}  # Move directions\n",
    "        self.q_table = {}\n",
    "\n",
    "    '''\n",
    "    obtain the q-value\n",
    "    '''\n",
    "    def get_q_value(self, state, action):\n",
    "        return self.q_table.get((state, action), 0.0)\n",
    "\n",
    "    '''\n",
    "    update the q-value of state S with action A\n",
    "    '''\n",
    "    def update_q_value(self, state, action, value):\n",
    "        self.q_table[(state, action)] = value\n",
    "\n",
    "    '''\n",
    "    when epsilon is large than uniform number, the agent is exploring and choose the action randomly\n",
    "\n",
    "    else: \n",
    "        calculate the q-values of each action at state S\n",
    "        find the best q-value\n",
    "        find the best action at state S\n",
    "    '''\n",
    "    def choose_action(self, state):\n",
    "        # exploring \n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(self.actions)\n",
    "        else:\n",
    "            # q-values for all actions at state S\n",
    "            q_values = [self.get_q_value(state, action) for action in self.actions]\n",
    "\n",
    "            # the max q-value within these actions\n",
    "            max_q_value = np.max(q_values)\n",
    "\n",
    "            # based on the best q-value, find the best action\n",
    "            best_actions = [action for action, q_value in zip(self.actions, q_values) if q_value == max_q_value]\n",
    "\n",
    "            return random.choice(best_actions)\n",
    "\n",
    "\n",
    "    '''\n",
    "    We use temporal-difference (TD) learning algorithm\n",
    "\n",
    "    Q(S,A) = Q(S,A) + alpha * (Reward + gamma* Q(S',A') - Q(S,A))\n",
    "    \n",
    "    \n",
    "    Features: \n",
    "        TD has low variance, some bias\n",
    "        TD converges to V(s) (with value tables)\n",
    "        TD converges faster than MC(Monte-Carlo)\n",
    "        TD is more sensitive to initial value\n",
    "    '''\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        q_value = self.get_q_value(state, action)\n",
    "        next_q_values = [self.get_q_value(next_state, next_action) for next_action in self.actions]\n",
    "        max_next_q_value = max(next_q_values)\n",
    "\n",
    "        # TD target = Reward of next state + discount factor * Max value of next q value        \n",
    "        td_target = reward + self.gamma * max_next_q_value\n",
    "        # TD error = TD target - V(St)\n",
    "        td_error = td_target - q_value\n",
    "\n",
    "        # calculate the new value of q at state S with action A\n",
    "        new_q_value = q_value + self.alpha * td_error\n",
    "        self.update_q_value(state, action, new_q_value)\n",
    "    \n",
    "    \n",
    "\n",
    "    '''\n",
    "    how the agent moves with the input action\n",
    "    '''\n",
    "    def move(self, action):\n",
    "        # how the position of agent changes with the input action\n",
    "        direction = self.action_dir[action]\n",
    "        # how far the agent moves on x and y\n",
    "        dy, dx = direction\n",
    "        # the position cannot exceed the boundary \n",
    "        new_position = (min(max(self.grid.agent_position[0] + dy, 0), self.grid.n-1), \n",
    "                        min(max(self.grid.agent_position[1] + dx, 0), self.grid.n-1)) \n",
    "\n",
    "        # Update agent position and handle item pickup\n",
    "        self.grid.agent_position = new_position\n",
    "        if self.grid.agent_position == self.grid.item_position and not self.grid.has_item:\n",
    "            self.grid.has_item = True\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    define the reward rules \n",
    "        1. general: reward = -1 : the agent moves one step, the reward is -1\n",
    "        2. agent arrives the target: \n",
    "            agent arrives with item\n",
    "            agent arrives without item\n",
    "        3. item is picked / isn't picked\n",
    "        \n",
    "    '''\n",
    "    def reward_function(self):\n",
    "        reward = -1\n",
    "\n",
    "        # agent arrives at the target with item\n",
    "        if self.grid.is_at_target():\n",
    "            return 50\n",
    "        \n",
    "        # agent arrives at the target\n",
    "        elif self.grid.agent_position == self.grid.target_position:\n",
    "            return -50\n",
    "\n",
    "        # the agent has not picked the item\n",
    "        if not self.grid.has_item:\n",
    "            new_dist_item = self.grid._dist(self.grid.agent_position, self.grid.item_position)\n",
    "            if new_dist_item < self.grid.perv_dist_item:\n",
    "                reward += 5\n",
    "            else:\n",
    "                reward -= 5\n",
    "        \n",
    "        # the agent picks the item\n",
    "        else:\n",
    "            new_dist_target = self.grid._dist(self.grid.agent_position, self.grid.target_position)\n",
    "            if new_dist_target < self.grid.perv_dist_target:\n",
    "                reward += 10\n",
    "            else:\n",
    "                reward -= 10\n",
    "\n",
    "        self.grid.update_distances()\n",
    "        return reward\n",
    "    \n",
    "\n",
    "    '''\n",
    "    update the epsilon and alpha with decay rate\n",
    "    '''\n",
    "    def update_parameters(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "        self.alpha = max(self.alpha_min, self.alpha * self.alpha_decay)\n",
    "\n",
    "\n",
    "    '''\n",
    "    used for testing and visualizing the q table\n",
    "    '''\n",
    "    def print_q_table(self):\n",
    "        for key in sorted(self.q_table.keys(), key=lambda x: str(x[0])):\n",
    "            state, action = key\n",
    "            print(f\"State: {state}, Action: {action}, Q-value: {self.q_table[key]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentTrainer:\n",
    "    def __init__(self, grid, agent, episodes=1000, reward_smoothing_window=10, step_smoothing_window=10):\n",
    "        self.grid = grid\n",
    "        self.agent = agent\n",
    "        self.episodes = episodes\n",
    "        self.reward_smoothing_window = reward_smoothing_window\n",
    "        self.step_smoothing_window = step_smoothing_window\n",
    "\n",
    "        # Metrics storage\n",
    "        self.step_counts = []\n",
    "        self.total_rewards = []\n",
    "        self.q_value_convergence = []\n",
    "        self.policy_stability = []\n",
    "        self.exploration_ratios = []\n",
    "        self.learning_curve_stability = []\n",
    "\n",
    "        # Recent metrics for smoothing\n",
    "        self.recent_rewards = []\n",
    "        self.recent_steps = []\n",
    "\n",
    "    def train(self):\n",
    "        for episode in range(self.episodes):\n",
    "            state = self.grid.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            steps = 0\n",
    "            old_q_table = self.agent.q_table.copy()\n",
    "            exploration_count = 0\n",
    "\n",
    "            while not done:\n",
    "                # Track exploration vs exploitation\n",
    "                if np.random.uniform(0, 1) < self.agent.epsilon:\n",
    "                    exploration_count += 1\n",
    "\n",
    "                action = self.agent.choose_action(state)\n",
    "                self.agent.move(action)\n",
    "                reward = self.agent.reward_function()\n",
    "\n",
    "                next_state = self.grid.get_state()\n",
    "                done = self.grid.is_at_target()\n",
    "                self.agent.update_q_table(state, action, reward, next_state)\n",
    "                state = next_state\n",
    "\n",
    "                total_reward += reward\n",
    "                steps += 1\n",
    "\n",
    "            # Store metrics after the episode\n",
    "            self.exploration_ratios.append(self.calculate_exploration_ratio(exploration_count, steps))\n",
    "            self.total_rewards.append(self.calculate_smoothed_reward(total_reward))\n",
    "            self.step_counts.append(self.calculate_smoothed_steps(steps))\n",
    "            self.learning_curve_stability.append(self.calculate_learning_curve_stability())\n",
    "            self.q_value_convergence.append(self.calculate_q_value_convergence())\n",
    "            self.policy_stability.append(self.calculate_policy_stability(old_q_table))\n",
    "\n",
    "            # Update parameters for smoother convergence\n",
    "            self.agent.update_parameters()\n",
    "\n",
    "        return self.step_counts, self.total_rewards, self.q_value_convergence, self.policy_stability, self.exploration_ratios, self.learning_curve_stability\n",
    "\n",
    "    # Calculate exploration vs exploitation ratio\n",
    "    def calculate_exploration_ratio(self, exploration_count, steps):\n",
    "        return exploration_count / steps if steps > 0 else 0\n",
    "\n",
    "    # Calculate smoothed reward\n",
    "    def calculate_smoothed_reward(self, total_reward):\n",
    "        self.recent_rewards.append(total_reward)\n",
    "        if len(self.recent_rewards) > self.reward_smoothing_window:\n",
    "            self.recent_rewards.pop(0)  # Maintain a fixed-size window\n",
    "        return np.mean(self.recent_rewards)\n",
    "\n",
    "    # Calculate smoothed steps\n",
    "    def calculate_smoothed_steps(self, steps):\n",
    "        self.recent_steps.append(steps)\n",
    "        if len(self.recent_steps) > self.step_smoothing_window:\n",
    "            self.recent_steps.pop(0)  # Maintain a fixed-size window\n",
    "        return np.mean(self.recent_steps)\n",
    "\n",
    "    # Calculate learning curve stability (variance of rewards)\n",
    "    def calculate_learning_curve_stability(self):\n",
    "        return np.var(self.recent_rewards)\n",
    "\n",
    "    # Calculate Q-value convergence\n",
    "    def calculate_q_value_convergence(self):\n",
    "        q_values = list(self.agent.q_table.values())\n",
    "        return np.mean(q_values)\n",
    "\n",
    "    # Calculate policy stability\n",
    "    def calculate_policy_stability(self, old_q_table):\n",
    "        policy_changes = sum(\n",
    "            1 for key in self.agent.q_table if key in old_q_table and self.agent.q_table[key] != old_q_table[key]\n",
    "        )\n",
    "        return policy_changes / (len(self.agent.q_table) if self.agent.q_table else 1)\n",
    "\n",
    "    # Plot steps and rewards together\n",
    "    def plot_reward_and_steps(self):\n",
    "        fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "        # Plot steps on the left Y-axis\n",
    "        ax1.set_xlabel('Episode')\n",
    "        ax1.set_ylabel('Steps', color='red')\n",
    "        ax1.plot(range(len(self.step_counts)), self.step_counts, color='red', label='Steps')\n",
    "        ax1.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "        # Plot rewards on the right Y-axis\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.set_ylabel('Total Reward', color='blue')\n",
    "        ax2.plot(range(len(self.total_rewards)), self.total_rewards, color='blue', label='Total Reward')\n",
    "        ax2.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "        # Title and layout\n",
    "        plt.title('Steps and Total Rewards per Episode')\n",
    "        fig.tight_layout()\n",
    "\n",
    "        # Show plot\n",
    "        plt.show()\n",
    "\n",
    "    # Plot exploration vs exploitation ratio\n",
    "    def plot_exploration_vs_exploitation(self):\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.plot(range(len(self.exploration_ratios)), self.exploration_ratios, color='purple')\n",
    "        ax.set_xlabel('Episode')\n",
    "        ax.set_ylabel('Exploration vs Exploitation Ratio')\n",
    "        ax.set_title('Exploration vs Exploitation Over Episodes')\n",
    "        plt.show()\n",
    "\n",
    "    # Plot learning curve stability\n",
    "    def plot_learning_curve_stability(self):\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.plot(range(len(self.learning_curve_stability)), self.learning_curve_stability, color='blue')\n",
    "        ax.set_xlabel('Episode')\n",
    "        ax.set_ylabel('Learning Curve Stability (Variance of Rewards)')\n",
    "        ax.set_title('Learning Curve Stability Over Episodes')\n",
    "        plt.show()\n",
    "\n",
    "    # Plot Q-value convergence vs policy stability\n",
    "    def plot_q_convergence_vs_policy_stability(self):\n",
    "        fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "        # Plot Q-value convergence on the left Y-axis\n",
    "        ax1.set_xlabel('Episode')\n",
    "        ax1.set_ylabel('Q-value Convergence', color='green')\n",
    "        ax1.plot(range(len(self.q_value_convergence)), self.q_value_convergence, color='green', label='Q-value Convergence')\n",
    "        ax1.tick_params(axis='y', labelcolor='green')\n",
    "\n",
    "        # Plot policy stability on the right Y-axis\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.set_ylabel('Policy Stability', color='red')\n",
    "        ax2.plot(range(len(self.policy_stability)), self.policy_stability, color='red', label='Policy Stability')\n",
    "        ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "        # Title and layout\n",
    "        plt.title('Q-value Convergence and Policy Stability per Episode')\n",
    "        fig.tight_layout()\n",
    "\n",
    "        # Show plot\n",
    "        plt.show()\n",
    "\n",
    "    # Display the final Q-table\n",
    "    def display_q_table(self):\n",
    "        # Create a Q-table for all possible states and actions\n",
    "        q_table_array = np.zeros((self.grid.n, self.grid.n, len(self.agent.actions)))\n",
    "        \n",
    "        for (state, action), q_value in self.agent.q_table.items():\n",
    "            agent_pos, item_pos, has_item = state\n",
    "            action_index = self.agent.actions.index(action)\n",
    "            q_table_array[agent_pos[0], agent_pos[1], action_index] = q_value\n",
    "        \n",
    "        # Print Q-values for each state\n",
    "        for i in range(self.grid.n):\n",
    "            for j in range(self.grid.n):\n",
    "                print(f\"State ({i}, {j}): \", q_table_array[i, j])\n",
    "        print(\"\\nActions: \", self.agent.actions)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation(grid, agent, episodes=1000):\n",
    "    global stop, fig, ax, mat, bnext, bstart, bstop, binit, sspeed, speed, time\n",
    "    stop = True\n",
    "    speed = 1.0\n",
    "    time = 0\n",
    "\n",
    "    def stopAnim(event):\n",
    "        global stop\n",
    "        stop = True\n",
    "\n",
    "    def startAnim(event):\n",
    "        global stop\n",
    "        stop = False\n",
    "        animate()\n",
    "\n",
    "    def advance(event):\n",
    "        global time, stop\n",
    "        time += 1\n",
    "        state = grid.get_state()\n",
    "        action = agent.choose_action(state)\n",
    "        agent.move(action)\n",
    "        reward = agent.reward_function()\n",
    "        next_state = grid.get_state()\n",
    "        agent.update_q_table(state, action, reward, next_state)\n",
    "        mat.set_data(render_grid(grid))\n",
    "        plt.title(f't = {time}')\n",
    "        plt.draw()\n",
    "\n",
    "        # Check if agent has reached the target with the item\n",
    "        if grid.has_item and grid.agent_position == grid.target_position:\n",
    "            print(\"Agent reached the target with the item!\")\n",
    "            stop = True  # Stop the animation\n",
    "\n",
    "    def initAnim(event):\n",
    "        global time\n",
    "        time = 0\n",
    "        grid.reset()\n",
    "        mat.set_data(render_grid(grid))\n",
    "        plt.title(f't = {time}')\n",
    "        plt.draw()\n",
    "\n",
    "    def updateSpeed(val):\n",
    "        global speed\n",
    "        speed = 1 / sspeed.val\n",
    "\n",
    "    def render_grid(grid):\n",
    "        grid_display = np.zeros((grid.n, grid.n))\n",
    "        ay, ax = grid.agent_position\n",
    "        iy, ix = grid.item_position\n",
    "        ty, tx = grid.target_position\n",
    "        grid_display[ay, ax] = 1  # Agent\n",
    "        grid_display[iy, ix] = 2  # Item\n",
    "        grid_display[ty, tx] = 3  # Target\n",
    "        if grid.has_item:\n",
    "            grid_display[ay, ax] = 4  # Agent with item\n",
    "        return grid_display\n",
    "\n",
    "    def animate():\n",
    "        global stop\n",
    "        advance(None)\n",
    "        if not stop:\n",
    "            threading.Timer(speed, animate).start()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.axis('off')\n",
    "    plt.title(\"GridWorld Q-learning Simulation\")\n",
    "\n",
    "    # Add legend\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='blue', label='Agent'),\n",
    "        Patch(facecolor='green', label='Item'),\n",
    "        Patch(facecolor='yellow', label='Target'),\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "    axspeed = plt.axes([0.175, 0.05, 0.65, 0.03])\n",
    "    sspeed = Slider(axspeed, 'Speed', 0.1, 10.0, valinit=1.0)\n",
    "    sspeed.on_changed(updateSpeed)\n",
    "\n",
    "    axnext = plt.axes([0.85, 0.15, 0.1, 0.075])\n",
    "    axstart = plt.axes([0.85, 0.25, 0.1, 0.075])\n",
    "    axstop = plt.axes([0.85, 0.35, 0.1, 0.075])\n",
    "    axinit = plt.axes([0.85, 0.45, 0.1, 0.075])\n",
    "    bnext = Button(axnext, 'Next')\n",
    "    bnext.on_clicked(advance)\n",
    "    bstart = Button(axstart, 'Start')\n",
    "    bstart.on_clicked(startAnim)\n",
    "    bstop = Button(axstop, 'Stop')\n",
    "    bstop.on_clicked(stopAnim)\n",
    "    binit = Button(axinit, 'Init')\n",
    "    binit.on_clicked(initAnim)\n",
    "\n",
    "    mat = ax.matshow(render_grid(grid), cmap=plt.cm.viridis)\n",
    "    initAnim(None)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid command name \"4353569536process_stream_events\"\n",
      "    while executing\n",
      "\"4353569536process_stream_events\"\n",
      "    (\"after\" script)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State (0, 0):  [-0.12433649  0.04654218  0.16406426 -0.07456806]\n",
      "State (0, 1):  [-0.06        0.20088081  0.47570478 -0.06      ]\n",
      "State (0, 2):  [-0.20525348  0.23249948 -0.20080653  0.72918935]\n",
      "State (0, 3):  [-0.09677265 -0.19513267 -0.20080653  0.05233552]\n",
      "State (0, 4):  [-0.11707808  0.07805205 -0.11707808  0.26899031]\n",
      "State (1, 0):  [-0.07612603  0.15748557  0.04       -0.13471914]\n",
      "State (1, 1):  [-0.06        0.04794888  0.07932063 -0.35178659]\n",
      "State (1, 2):  [-0.10957918  0.28553312  0.85398554  0.16989507]\n",
      "State (1, 3):  [-0.4070374   0.30622577 -0.10120106 -0.41846627]\n",
      "State (1, 4):  [ 0.11401653  0.16904924 -0.23033486  0.08335179]\n",
      "State (2, 0):  [-0.2285379  -0.23483494  0.04319334 -0.24096388]\n",
      "State (2, 1):  [ 0.03116514 -0.28126094 -0.30015671 -0.28586433]\n",
      "State (2, 2):  [0.26966233 0.12356236 1.00941097 0.43823223]\n",
      "State (2, 3):  [-0.11043487 -0.11021867 -0.34080068  0.08923928]\n",
      "State (2, 4):  [ 0.0884723  -0.09872923  0.04776408  0.74339309]\n",
      "State (3, 0):  [ 0.07711181 -0.09390552  0.1540927  -0.32412443]\n",
      "State (3, 1):  [ 0.65212571 -0.1706556   0.14985011 -0.18784451]\n",
      "State (3, 2):  [ 0.2386883  -0.13623411 -0.20219005  0.21572679]\n",
      "State (3, 3):  [-0.05999627  0.84085299 -0.76497576 -0.06331008]\n",
      "State (3, 4):  [-0.14457893 -1.8296721  -0.22017469  0.04768957]\n",
      "State (4, 0):  [ 0.05775974 -0.0799889   0.04311351 -0.06      ]\n",
      "State (4, 1):  [ 0.04403267 -0.13539612  0.06162842 -0.18943483]\n",
      "State (4, 2):  [ 0.09395726 -0.06071544  0.08054432 -0.18943483]\n",
      "State (4, 3):  [ 0.0441384  -0.06071544 -0.50596201 -0.06032632]\n",
      "State (4, 4):  [ 0.0439684  -0.5        -0.5         0.04373253]\n",
      "\n",
      "Actions:  ['n', 's', 'e', 'w']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize the grid and agent\n",
    "    grid = Grid(n=5)\n",
    "    agent = Agent(actions=['n', 's', 'e', 'w'], alpha=0.1, gamma=0.95, grid=grid, epsilon=1, epsilon_decay=0.9)\n",
    "    \n",
    "    # Initialize the trainer\n",
    "    trainer = AgentTrainer(grid, agent, episodes=500)\n",
    "    \n",
    "    # Train the agent\n",
    "    trainer.train()\n",
    "\n",
    "    # Run the dynamic simulation\n",
    "    simulation(grid, agent)\n",
    "    \n",
    "    # Plotting reward and steps on the same graph\n",
    "    trainer.plot_reward_and_steps()\n",
    "\n",
    "    # Plotting other metrics\n",
    "    trainer.plot_exploration_vs_exploitation()\n",
    "    trainer.plot_learning_curve_stability()\n",
    "    trainer.plot_q_convergence_vs_policy_stability()\n",
    "\n",
    "    # Display the final Q-table\n",
    "    trainer.display_q_table()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
